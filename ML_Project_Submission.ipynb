{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import cm\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from scipy.optimize import fmin_powell\n",
    "from ml_metrics import quadratic_weighted_kappa\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set output options for viewing data\n",
    "pd.set_option(\"display.max_rows\",150)\n",
    "pd.get_option(\"display.max_rows\")\n",
    "pd.set_option(\"display.max_columns\",250)\n",
    "pd.get_option(\"display.max_columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the train and the test dataset\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature generation, only the following feature generation improves the predicitve capacity of the model\n",
    "# here are some of the not helpful feature generations: summing up variables of a particular \"type\", \n",
    "# where a number of variables with identical names, except the digit at the end, are thought to belong to a type. e.g., \"Medical_History_{any digit from 1 to 41}\" \n",
    "train[\"BMI_age\"] = train[\"BMI\"]*train[\"Ins_Age\"]\n",
    "test[\"BMI_age\"] = test[\"BMI\"]*test[\"Ins_Age\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Processing data and creating dataframes varible-type-wise from train and test data \n",
    "CATEGORICAL_COLUMNS = [\"Product_Info_1\", \"Product_Info_2\", \"Product_Info_3\", \"Product_Info_5\", \"Product_Info_6\",\\\n",
    "                       \"Product_Info_7\", \"Employment_Info_2\", \"Employment_Info_3\", \"Employment_Info_5\", \"InsuredInfo_1\",\\\n",
    "                       \"InsuredInfo_2\", \"InsuredInfo_3\", \"InsuredInfo_4\", \"InsuredInfo_5\", \"InsuredInfo_6\", \"InsuredInfo_7\",\\\n",
    "                       \"Insurance_History_1\", \"Insurance_History_2\", \"Insurance_History_3\", \"Insurance_History_4\", \"Insurance_History_7\",\\\n",
    "                       \"Insurance_History_8\", \"Insurance_History_9\", \"Family_Hist_1\", \"Medical_History_2\", \"Medical_History_3\",\\\n",
    "                       \"Medical_History_4\", \"Medical_History_5\", \"Medical_History_6\", \"Medical_History_7\", \"Medical_History_8\",\\\n",
    "                       \"Medical_History_9\", \"Medical_History_11\", \"Medical_History_12\", \"Medical_History_13\", \"Medical_History_14\",\\\n",
    "                       \"Medical_History_16\", \"Medical_History_17\", \"Medical_History_18\", \"Medical_History_19\", \"Medical_History_20\",\\\n",
    "                       \"Medical_History_21\", \"Medical_History_22\", \"Medical_History_23\", \"Medical_History_25\", \"Medical_History_26\",\\\n",
    "                       \"Medical_History_27\", \"Medical_History_28\", \"Medical_History_29\", \"Medical_History_30\", \"Medical_History_31\",\\\n",
    "                       \"Medical_History_33\", \"Medical_History_34\", \"Medical_History_35\", \"Medical_History_36\", \"Medical_History_37\",\\\n",
    "                       \"Medical_History_38\", \"Medical_History_39\", \"Medical_History_40\", \"Medical_History_41\"]\n",
    "CONTINUOUS_COLUMNS = [\"Product_Info_4\", \"Ins_Age\", \"Ht\", \"Wt\", \"BMI\",\"BMI_age\",\n",
    "                      \"Employment_Info_1\", \"Employment_Info_4\", \"Employment_Info_6\",\n",
    "                      \"Insurance_History_5\", \"Family_Hist_2\", \"Family_Hist_3\", \"Family_Hist_4\", \"Family_Hist_5\"]\n",
    "DISCRETE_COLUMNS = [\"Medical_History_1\", \"Medical_History_10\", \"Medical_History_15\", \"Medical_History_24\", \"Medical_History_32\"]\n",
    "DUMMY_COLUMNS = [\"Medical_Keyword_{}\".format(i) for i in range(1, 49)]\n",
    "\n",
    "categorical_data = pd.concat([train[CATEGORICAL_COLUMNS], test[CATEGORICAL_COLUMNS]])\n",
    "continuous_data = pd.concat([train[CONTINUOUS_COLUMNS], test[CONTINUOUS_COLUMNS]])\n",
    "discrete_data = pd.concat([train[DISCRETE_COLUMNS], test[DISCRETE_COLUMNS]])\n",
    "dummy_data = pd.concat([train[DUMMY_COLUMNS], test[DUMMY_COLUMNS]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before dropiing: (79146, 48)\n",
      "after dropiing: (79146, 46)\n"
     ]
    }
   ],
   "source": [
    "# feature reduction based on chi-square test. \n",
    "# drop columns, Medical_Keyword_32 and 45 from dummy data\n",
    "categorical_data.shape\n",
    "#list(categorical_data)\n",
    "print(\"before dropiing:\",dummy_data.shape)\n",
    "dummy_data = dummy_data.drop([\"Medical_Keyword_32\",\"Medical_Keyword_45\"], axis = 1)\n",
    "print(\"after dropiing:\",dummy_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scaling and clipping not needed for this dataset. Continuous variables were already normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scaled descrete data, normalized from 0 to 1. \n",
    "scaled_discrete_data = discrete_data.sub(discrete_data.min()).div((discrete_data.max() - discrete_data.min()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = train['Response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converted categorical variables into dummy variables. \n",
    "categorical_data_Product_Info_2 = pd.get_dummies(categorical_data[\"Product_Info_2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merged independent features of test and train datasets\n",
    "X = pd.concat([categorical_data,categorical_data_Product_Info_2,scaled_discrete_data, continuous_data, dummy_data], axis = 1)\n",
    "X = X.drop([\"Product_Info_2\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imputation: only contious and descrete data had missing values. not missing values in any categorical features.\n",
    "X=X.fillna(X.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.isnull().sum().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the train and test datasets again for ML \n",
    "X_train = X[:len(y)]\n",
    "X_test = X[len(y):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# XGBoost Model \n",
    "clf = xgb.XGBClassifier(\n",
    "                    max_depth=3,\n",
    "                    learning_rate=0.2,\n",
    "                    gamma=0.0,\n",
    "                    min_child_weight=1,\n",
    "                    max_delta_step=0.0,\n",
    "                    subsample=1.0,\n",
    "                    colsample_bytree=1.0,\n",
    "                    colsample_bylevel=1.0,\n",
    "                    reg_alpha=0.0,\n",
    "                    reg_lambda=1.0,\n",
    "                    n_estimators=1000,\n",
    "                    silent=0,\n",
    "                    scale_pos_weight=1.0,\n",
    "                    base_score=0.5,\n",
    "                    seed=1337,\n",
    "                    missing=None,\n",
    "                    booster='gbtree',\n",
    "                    objective='multi:softprob',\n",
    "                    early_stopping_rounds=10\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44min 18s, sys: 21.3 s, total: 44min 39s\n",
      "Wall time: 47min 3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,\n",
       "       colsample_bytree=1.0, early_stopping_rounds=10, gamma=0.0,\n",
       "       learning_rate=0.2, max_delta_step=0.0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=1000, n_jobs=1,\n",
       "       nthread=None, objective='multi:softprob', random_state=0,\n",
       "       reg_alpha=0.0, reg_lambda=1.0, scale_pos_weight=1.0, seed=1337,\n",
       "       silent=0, subsample=1.0)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the XGBoost Model\n",
    "%time clf.fit(X_train,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prob_z=clf.predict_proba(X_test)\n",
    "prob_z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict the target variable\n",
    "z = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 8, 6, 8, 8, 8, 8, 8, 7, 8, 8, 8, 4, 8, 6, 8, 7, 8, 8, 2, 8, 8, 8,\n",
       "       8, 6, 7, 8, 8, 1, 8, 7, 4, 5, 8, 3, 4, 8, 8, 6, 8, 2, 6, 7, 8, 7, 3,\n",
       "       8, 6, 8, 6])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape\n",
    "z[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# saving the target variable and submitting the csv file\n",
    "df = pd.DataFrame(data={'Id' : test['Id'], 'Response' : z})\n",
    "df.to_csv('submit_xgboost_758pm_june6.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ridge regression on cleaned data\n",
    "model = Ridge(alpha=1)\n",
    "model.fit(X_train, y)\n",
    "z_ridge = model.predict(X_test)\n",
    "\n",
    "z_ridge = np.round(z_ridge)\n",
    "z_ridge[z_ridge < 1] = 1\n",
    "z_ridge[z_ridge > 8] = 8\n",
    "z_ridge = z_ridge.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={'Id' : test['Id'], 'Response' : z_ridge})\n",
    "df.to_csv('submit_ridge_alpha1_812pm_june6.csv', index=False) # score: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting logistic regression to the final model\n",
    "lr = linear_model.LogisticRegression()\n",
    "lr.fit(X_train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z_lg = lr.predict(X_test)\n",
    "plt.hist(z_lg)\n",
    "df = pd.DataFrame(data={'Id' : test['Id'], 'Response' : z_lg})\n",
    "df.to_csv('submit_lg_818pm_june6.csv', index=False) # score: 0.50426 at 9:08 pm on June 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# fitting multinomial logistic regression to the basic model\n",
    "mul_lr = linear_model.LogisticRegression(multi_class='multinomial', solver='newton-cg',max_iter=200).fit(X_train,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z_mul_lg = lr.predict(X_test)\n",
    "df1 = pd.DataFrame(data={'Id' : test['Id'], 'Response' : z_mul_lg})\n",
    "df1.to_csv('submit_mul_lg_842pm_june6.csv', index=False) # score: 0.50426 at 9:08 pm on June 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
